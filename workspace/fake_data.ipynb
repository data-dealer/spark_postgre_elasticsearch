#{"dirpath":"spark_postgre_elasticsearch/workspace","filename":"fake_data.ipynb"}
{"cells":[{"cell_type":"code","execution_count":null,"id":"bc797bbe-f0ff-48ec-9254-299a8cd9cb49","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.sql.types import *\n","\n","# Py4JJavaError: An error occurred while calling o73.save.\n","# : java.lang.NoClassDefFoundError: org/apache/commons/httpclient/protocol/ProtocolSocketFactory\n","spark = (\n","    SparkSession\n","    .builder\n","    .master('spark://host.docker.internal:7077')\n","    .appName(\"fake_data\")\n","    .config(\"spark.executor.instances\", 6)\n","    # .config(\"spark.default.parallelism\", 12)\n","    .getOrCreate()\n",")"]},{"cell_type":"code","execution_count":null,"id":"c452f8f7-dda8-43bd-9c8f-6ff1719be180","metadata":{},"outputs":[],"source":["import random\n","from faker import Faker\n","from pyspark.sql import SparkSession\n","\n","fake = Faker()\n","\n","def random_int(min_value, max_value):\n","  return random.randint(min_value, max_value)\n","    \n","data = [\n","    (fake.uuid4(),\n","     fake.name(),\n","     random_int(20, 65),\n","     fake.city(),\n","     fake.job(),\n","     random_int(50000, 200000),\n","     random.choice([\"Male\", \"Female\"]),\n","     random_int(160, 190),\n","     random_int(50, 100),\n","     random.choice([True, False]),\n","     fake.date_of_birth(),\n","     fake.color_name(),\n","     fake.text(),\n","     \"1\",\n","    )\n","    for _ in range(100000)]\n","\n","# Create DataFrame with 12 Columns\n","df = spark.createDataFrame(data, schema=[\n","    \"id\", \"name\", \"age\", \"city\", \"department\", \"salary\", \"gender\", \n","    \"height\", \"weight\", \"isActive\", \"join_date\", \"favorite_color\",\n","    \"note\", \"batch\"\n","])"]},{"cell_type":"code","execution_count":null,"id":"eeeb623a-9866-4ee4-844b-cb6ba603973f","metadata":{},"outputs":[],"source":["import functools\n","from pyspark.sql import DataFrame\n","\n","final_df = functools.reduce(DataFrame.unionAll ,[df.withColumn('batch', F.lit(i)) for i in range(1, 101, 1)])\n","final_df = final_df.withColumn('id', F.expr(\"concat(batch, '-', id)\"))"]},{"cell_type":"code","execution_count":null,"id":"2845f883-127d-419a-bd47-6e142cda2e71","metadata":{},"outputs":[],"source":["# final_df.show()"]},{"cell_type":"code","execution_count":null,"id":"7e7a09b0-cfc5-4591-a50d-ec198b5c4091","metadata":{},"outputs":[],"source":["# df = spark.read.parquet(\"s3a://datalake/fake_data_1M\")\n","# df = df.withColumn('id', F.expr(\"concat(batch, '-', id)\"))"]},{"cell_type":"code","execution_count":null,"id":"577b4276-8fe7-4cf2-92d1-aaee39f993e7","metadata":{},"outputs":[],"source":["(\n","    final_df.write\n","    .partitionBy(\"batch\")\n","    .format(\"parquet\")\n","    .mode(\"overwrite\")\n","    .save(\"s3a://datalake/fake_data_1M\")\n",")"]},{"cell_type":"code","execution_count":null,"id":"fd815579-3083-45b2-9926-e038c32af8d0","metadata":{},"outputs":[],"source":["# output 1587.1 MiB\t"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":5}
