spark.executor.memory=1G
spark.executor.cores=1
spark.executor.instances=2
spark.driver.memory=1G
spark.driver.cores=1
spark.cores.max=12
spark.default.parallelism=12
spark.sql.shuffle.partitions=100
spark.dynamicAllocation.enabled=false
log4j.logger.org.apache.hadoop.metrics2=ERROR
spark.ui.reverseProxy=true


# spark.history.fs.logDirectory=s3a://datalake/artifacts/sparklogs
# spark.eventLog.enabled=true
# spark.eventLog.dir=3a://artifacts

# # delta
spark.jars.packages=org.apache.hadoop:hadoop-aws:3.3.1,\
    org.elasticsearch:elasticsearch-spark-30_2.12:7.12.0

# org.apache.hadoop:hadoop-aws:3.3.1,\
#     io.delta:delta-core_2.12:2.3.0,\
#     org.apache.kafka:kafka-clients:3.2.1,\
#     org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,\
#     org.mlflow:mlflow-spark_2.12:2.11.3,\


# ,com.amazon.deequ:deequ:jar:2.0.3-spark-3.3
# ,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1
# spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
# spark.sql.catalog.datalake=org.apache.spark.sql.delta.catalog.DeltaCatalog

# hive
spark.sql.warehouse.dir=s3a://datalake

# hadoop s3a
spark.hadoop.fs.s3a.committer.name=directory
spark.hadoop.fs.s3a.committer.staging.tmp.path=/tmp/spark-meta/committer_staging
spark.hadoop.fs.s3a.buffer.dir=/tmp/spark-meta/buffer
spark.hadoop.fs.s3a.committer.staging.conflict-mode=fail
spark.hadoop.fs.s3a.endpoint=http://minio-server:9000
spark.hadoop.fs.s3a.access.key=minio
spark.hadoop.fs.s3a.secret.key=minio123
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory
#{"dirpath":"spark_postgre_elasticsearch/spark","filename":"spark-defaults.conf"}
spark.master=spark://spark-master:7077
spark.executor.memory=1G
spark.executor.cores=1
spark.executor.instances=2
spark.driver.memory=1G
spark.driver.cores=1
spark.cores.max=12
spark.default.parallelism=12
spark.sql.shuffle.partitions=100
spark.dynamicAllocation.enabled=false
log4j.logger.org.apache.hadoop.metrics2=ERROR
spark.ui.reverseProxy=true


# spark.history.fs.logDirectory=s3a://datalake/artifacts/sparklogs
# spark.eventLog.enabled=true
# spark.eventLog.dir=3a://artifacts

# # delta
spark.jars.packages=org.apache.hadoop:hadoop-aws:3.3.1,\
    org.elasticsearch:elasticsearch-spark-30_2.12:7.12.0

# org.apache.hadoop:hadoop-aws:3.3.1,\
#     io.delta:delta-core_2.12:2.3.0,\
#     org.apache.kafka:kafka-clients:3.2.1,\
#     org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1,\
#     org.mlflow:mlflow-spark_2.12:2.11.3,\


# ,com.amazon.deequ:deequ:jar:2.0.3-spark-3.3
# ,org.mongodb.spark:mongo-spark-connector_2.12:3.0.1
# spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
# spark.sql.catalog.datalake=org.apache.spark.sql.delta.catalog.DeltaCatalog

# hive
spark.sql.warehouse.dir=s3a://datalake

# hadoop s3a
spark.hadoop.fs.s3a.committer.name=directory
spark.hadoop.fs.s3a.committer.staging.tmp.path=/tmp/spark-meta/committer_staging
spark.hadoop.fs.s3a.buffer.dir=/tmp/spark-meta/buffer
spark.hadoop.fs.s3a.committer.staging.conflict-mode=fail
spark.hadoop.fs.s3a.endpoint=http://minio-server:9000
spark.hadoop.fs.s3a.access.key=minio
spark.hadoop.fs.s3a.secret.key=minio123
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
spark.hadoop.mapreduce.outputcommitter.factory.scheme.s3a=org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory
